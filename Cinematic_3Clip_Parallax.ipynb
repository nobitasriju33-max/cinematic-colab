{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "645ff18e",
   "metadata": {},
   "source": [
    "\n",
    "# üé¨ Premium Cinematic 3‚ÄëClip Video (Colab / GCP Ready)\n",
    "\n",
    "This notebook turns **one photo** into a **cinematic 3‚Äëclip video** using a lightweight 2.5D parallax method (depth‚Äëaware warping), plus **zoom, pan, color grading, vignetting, lens glow, and slow‚Äëmotion**.  \n",
    "It works on **free Colab GPUs** and also scales great on **Google Cloud A100**.\n",
    "\n",
    "> If you specifically want *full body motion* driven by big AI models, use Runway/Pika (paid) or gated research models. This notebook focuses on a dependable, premium-looking result without gated model hurdles.\n",
    "\n",
    "---\n",
    "\n",
    "## What you get\n",
    "- **Clip 1 ‚Äì Intro (Wide ‚Üí gentle zoom‚Äëin)**  \n",
    "- **Clip 2 ‚Äì Mid (parallax pan + lens glow)**  \n",
    "- **Clip 3 ‚Äì Close (slow zoom‚Äëin + bokeh‚Äëlike blur + fade‚Äëto‚Äëblack)**\n",
    "\n",
    "You'll upload one image and receive `cinematic_video.mp4`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304e2bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ‚¨áÔ∏è Install dependencies (OpenCV, MoviePy, Depth model utils)\n",
    "!pip install -q opencv-python imageio[ffmpeg] moviepy timm torch torchvision --upgrade\n",
    "\n",
    "import os, math, cv2, numpy as np, torch, imageio\n",
    "from moviepy.editor import ImageSequenceClip, vfx, AudioFileClip, concatenate_videoclips\n",
    "from PIL import Image\n",
    "\n",
    "# MiDaS depth utils (DPT_Hybrid) from torch.hub\n",
    "import torch.hub\n",
    "midas = None\n",
    "transform = None\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"‚úÖ Device:\", device)\n",
    "\n",
    "def load_midas():\n",
    "    global midas, transform\n",
    "    if midas is None:\n",
    "        midas = torch.hub.load(\"intel-isl/MiDaS\", \"DPT_Hybrid\")\n",
    "        midas.to(device)\n",
    "        midas.eval()\n",
    "        midas_transforms = torch.hub.load(\"intel-isl/MiDaS\", \"transforms\")\n",
    "        transform = midas_transforms.dpt_transform\n",
    "    return midas, transform\n",
    "\n",
    "_ = load_midas()\n",
    "print(\"‚úÖ MiDaS depth model ready.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ‚¨ÜÔ∏è Upload your image\n",
    "from google.colab import files\n",
    "\n",
    "uploaded = files.upload()\n",
    "assert len(uploaded) > 0, \"Please upload a photo.\"\n",
    "image_path = list(uploaded.keys())[0]\n",
    "print(\"üì∑ Image:\", image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a9b74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title üõ† Helper functions: depth, parallax, grading\n",
    "\n",
    "def read_image_cv2(path):\n",
    "    img = cv2.imread(path, cv2.IMREAD_COLOR)\n",
    "    if img is None:\n",
    "        raise RuntimeError(\"Failed to read image.\")\n",
    "    return img\n",
    "\n",
    "@torch.no_grad()\n",
    "def estimate_depth(img_bgr):\n",
    "    midas, transform = load_midas()\n",
    "    img_rgb = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB)\n",
    "    input_img = Image.fromarray(img_rgb)\n",
    "    input_batch = transform(input_img).to(device)\n",
    "    prediction = midas(input_batch).squeeze().cpu().numpy()\n",
    "    # normalize to [0,1]\n",
    "    d = prediction\n",
    "    d = (d - d.min()) / (d.max() - d.min() + 1e-6)\n",
    "    return d\n",
    "\n",
    "def apply_parallax(img, depth, shift_x=10, shift_y=6):\n",
    "    \"\"\"Simple depth-aware parallax using warping.\n",
    "    Positive shift_x pans to the right (foreground moves more than background).\n",
    "    \"\"\"\n",
    "    h, w = depth.shape\n",
    "    yy, xx = np.meshgrid(np.arange(h), np.arange(w), indexing='ij')\n",
    "    # foreground (smaller depth) moves more ‚Üí use (1 - depth)\n",
    "    fg = 1.0 - depth\n",
    "    map_x = (xx - fg * shift_x).astype(np.float32)\n",
    "    map_y = (yy - fg * shift_y).astype(np.float32)\n",
    "    warped = cv2.remap(img, map_x, map_y, interpolation=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return warped\n",
    "\n",
    "def smoothstep(edge0, edge1, x):\n",
    "    t = np.clip((x - edge0) / (edge1 - edge0 + 1e-6), 0.0, 1.0)\n",
    "    return t * t * (3 - 2 * t)\n",
    "\n",
    "def cinematic_grade(img, intensity=0.18, lift=0.02, gamma=0.95, gain=1.12, vignette=0.35, glow=0.15):\n",
    "    imgf = img.astype(np.float32) / 255.0\n",
    "    # simple lift-gamma-gain\n",
    "    graded = np.clip((imgf + lift), 0, 1)\n",
    "    graded = np.power(graded, gamma)\n",
    "    graded = np.clip(graded * gain, 0, 1)\n",
    "    # gentle teal-orange split via channel bias\n",
    "    bias = np.array([1.06, 1.0, 0.94], dtype=np.float32)  # BGR bias (cool shadows, warm highs)\n",
    "    graded = np.clip(graded * bias, 0, 1)\n",
    "\n",
    "    # vignette\n",
    "    h, w = graded.shape[:2]\n",
    "    y, x = np.ogrid[:h, :w]\n",
    "    cx, cy = w / 2.0, h / 2.0\n",
    "    rx, ry = w * 0.75, h * 0.75\n",
    "    v = 1.0 - smoothstep(0.0, 1.0, ((x-cx)**2/(rx*rx) + (y-cy)**2/(ry*ry)))\n",
    "    v = (1 - vignette) + vignette * v\n",
    "    graded *= v[..., None]\n",
    "\n",
    "    # glow (bloom)\n",
    "    blur = cv2.GaussianBlur((graded*255).astype(np.uint8), (0,0), sigmaX=6, sigmaY=6)\n",
    "    blur = blur.astype(np.float32)/255.0\n",
    "    graded = np.clip(graded + glow * blur, 0, 1)\n",
    "\n",
    "    # overall intensity mix\n",
    "    out = (imgf*(1-intensity) + graded*intensity)\n",
    "    return np.clip(out*255, 0, 255).astype(np.uint8)\n",
    "\n",
    "def zoom_pan(img, t, zoom_start=1.0, zoom_end=1.12, pan_x=0.0, pan_y=0.0):\n",
    "    \"\"\"t in [0,1]\"\"\"\n",
    "    h, w = img.shape[:2]\n",
    "    z = zoom_start + (zoom_end - zoom_start)*t\n",
    "    M = np.array([[z, 0, -w*(z-1)/2 + pan_x*t],\n",
    "                  [0, z, -h*(z-1)/2 + pan_y*t]], dtype=np.float32)\n",
    "    out = cv2.warpAffine(img, M, (w, h), flags=cv2.INTER_LINEAR, borderMode=cv2.BORDER_REFLECT)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63deda3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title üéûÔ∏è Generate premium 3‚Äëclip cinematic sequence\n",
    "#@markdown You can tweak durations (seconds) and fps if you like.\n",
    "fps = 24 #@param {type:\"slider\", min:8, max:60, step:1}\n",
    "clip1_sec = 4 #@param {type:\"slider\", min:2, max:8, step:1}\n",
    "clip2_sec = 4 #@param {type:\"slider\", min:2, max:8, step:1}\n",
    "clip3_sec = 4 #@param {type:\"slider\", min:2, max:8, step:1}\n",
    "\n",
    "img = read_image_cv2(image_path)\n",
    "h, w, _ = img.shape\n",
    "\n",
    "# Estimate depth once\n",
    "depth = estimate_depth(img)\n",
    "\n",
    "frames = []\n",
    "\n",
    "# Clip 1: Wide ‚Üí gentle zoom-in with grading\n",
    "for i in range(int(clip1_sec*fps)):\n",
    "    t = i / max(1, (clip1_sec*fps - 1))\n",
    "    f = zoom_pan(img, t, zoom_start=1.0, zoom_end=1.08, pan_x=8, pan_y=6)\n",
    "    f = cinematic_grade(f, intensity=0.22, vignette=0.32, glow=0.12)\n",
    "    frames.append(f)\n",
    "\n",
    "# Clip 2: Parallax pan with lens glow\n",
    "for i in range(int(clip2_sec*fps)):\n",
    "    t = i / max(1, (clip2_sec*fps - 1))\n",
    "    shift_x = 14 * (t - 0.5)  # left to right\n",
    "    shift_y = 8 * math.sin(t*math.pi) * 0.5\n",
    "    f = apply_parallax(img, depth, shift_x=shift_x, shift_y=shift_y)\n",
    "    f = zoom_pan(f, t, zoom_start=1.02, zoom_end=1.10, pan_x=-10, pan_y=4)\n",
    "    f = cinematic_grade(f, intensity=0.26, vignette=0.38, glow=0.18)\n",
    "    frames.append(f)\n",
    "\n",
    "# Clip 3: Close, slow zoom + soft blur + fade to black\n",
    "for i in range(int(clip3_sec*fps)):\n",
    "    t = i / max(1, (clip3_sec*fps - 1))\n",
    "    f = zoom_pan(img, t, zoom_start=1.06, zoom_end=1.18, pan_x=4, pan_y=-6)\n",
    "    # bokeh-like softness\n",
    "    k = int(1 + 2* t * 2) * 2 + 1  # odd kernel grows slightly\n",
    "    f = cv2.GaussianBlur(f, (k, k), sigmaX=1.2 + 1.8*t)\n",
    "    f = cinematic_grade(f, intensity=0.28, vignette=0.42, glow=0.22)\n",
    "    # fade to black\n",
    "    fade = 1.0 - 0.8 * (t**1.5)\n",
    "    f = np.clip(f * fade, 0, 255).astype(np.uint8)\n",
    "    frames.append(f)\n",
    "\n",
    "# Save video\n",
    "video_path = \"cinematic_video.mp4\"\n",
    "clip = ImageSequenceClip([cv2.cvtColor(f, cv2.COLOR_BGR2RGB) for f in frames], fps=fps)\n",
    "clip.write_videofile(video_path, codec=\"libx264\", audio=False, verbose=False, logger=None)\n",
    "\n",
    "print(\"‚úÖ Done! Saved:\", video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4580cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#@title ‚¨áÔ∏è Download the video\n",
    "from google.colab import files\n",
    "files.download(\"cinematic_video.mp4\")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
